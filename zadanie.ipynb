{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  item_id  rating  timestamp\n",
      "0      196      242       3  881250949\n",
      "1      186      302       3  891717742\n",
      "2       22      377       1  878887116\n",
      "3      244       51       2  880606923\n",
      "4      166      346       1  886397596\n",
      "Liczba użytkowników: 943, Liczba filmów: 1682\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv('ml-100k/u.data', sep='\\t', names=columns)\n",
    "\n",
    "print(ratings.head())\n",
    "\n",
    "\n",
    "ratings = ratings.drop('timestamp', axis=1)\n",
    "\n",
    "num_users = ratings['user_id'].nunique()\n",
    "num_items = ratings['item_id'].nunique()\n",
    "print(f\"Liczba użytkowników: {num_users}, Liczba filmów: {num_items}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzenie macierzy użytkownik-film:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id  1     2     3     4     5     6     7     8     9     10    ...  \\\n",
      "user_id                                                              ...   \n",
      "1         5.0   3.0   4.0   3.0   3.0   5.0   4.0   1.0   5.0   3.0  ...   \n",
      "2         4.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   2.0  ...   \n",
      "3         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "4         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "5         4.0   3.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "\n",
      "item_id  1673  1674  1675  1676  1677  1678  1679  1680  1681  1682  \n",
      "user_id                                                              \n",
      "1         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "2         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "3         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "4         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "5         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "\n",
      "[5 rows x 1682 columns]\n"
     ]
    }
   ],
   "source": [
    "user_item_matrix = ratings.pivot(index='user_id', columns='item_id', values='rating')\n",
    "\n",
    "# Podgląd macierzy\n",
    "print(user_item_matrix.head())\n",
    "\n",
    "# Zapisanie macierzy do pliku CSV\n",
    "user_item_matrix.to_csv('user_item_matrix.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzenie danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1486126\n"
     ]
    }
   ],
   "source": [
    "print(user_item_matrix.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opcjonalna normlaizacja ocen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_normalized = user_item_matrix.subtract(user_item_matrix.mean(axis=1), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykład faktoryzacji macierzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 495926.8170\n",
      "Epoch 2/100, Loss: 121979.9677\n",
      "Epoch 3/100, Loss: 99580.0068\n",
      "Epoch 4/100, Loss: 94574.3085\n",
      "Epoch 5/100, Loss: 92290.0073\n",
      "Epoch 6/100, Loss: 90913.9078\n",
      "Epoch 7/100, Loss: 89882.2622\n",
      "Epoch 8/100, Loss: 88973.3422\n",
      "Epoch 9/100, Loss: 88087.9097\n",
      "Epoch 10/100, Loss: 87187.5830\n",
      "Epoch 11/100, Loss: 86270.6823\n",
      "Epoch 12/100, Loss: 85354.0916\n",
      "Epoch 13/100, Loss: 84457.0800\n",
      "Epoch 14/100, Loss: 83591.8038\n",
      "Epoch 15/100, Loss: 82761.8642\n",
      "Epoch 16/100, Loss: 81965.2196\n",
      "Epoch 17/100, Loss: 81197.4979\n",
      "Epoch 18/100, Loss: 80454.1103\n",
      "Epoch 19/100, Loss: 79731.2259\n",
      "Epoch 20/100, Loss: 79026.0898\n",
      "Epoch 21/100, Loss: 78337.0452\n",
      "Epoch 22/100, Loss: 77663.4341\n",
      "Epoch 23/100, Loss: 77005.4398\n",
      "Epoch 24/100, Loss: 76363.8906\n",
      "Epoch 25/100, Loss: 75740.0408\n",
      "Epoch 26/100, Loss: 75135.3458\n",
      "Epoch 27/100, Loss: 74551.2557\n",
      "Epoch 28/100, Loss: 73989.0445\n",
      "Epoch 29/100, Loss: 73449.6917\n",
      "Epoch 30/100, Loss: 72933.8173\n",
      "Epoch 31/100, Loss: 72441.6688\n",
      "Epoch 32/100, Loss: 71973.1464\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ustawienia\n",
    "num_users, num_items = user_item_matrix.shape\n",
    "num_features = 10  # Liczba ukrytych wymiarów\n",
    "learning_rate = 0.01\n",
    "reg_param = 0.1  # Parametr regularyzacji\n",
    "epochs = 100\n",
    "\n",
    "# Inicjalizacja macierzy P i Q\n",
    "P = np.random.normal(scale=1./num_features, size=(num_users, num_features))\n",
    "Q = np.random.normal(scale=1./num_features, size=(num_items, num_features))\n",
    "\n",
    "# Konwersja macierzy na NumPy\n",
    "ratings = user_item_matrix.to_numpy()\n",
    "\n",
    "# SGD\n",
    "for epoch in range(epochs):\n",
    "    for i in range(num_users):\n",
    "        for j in range(num_items):\n",
    "            if not np.isnan(ratings[i, j]):\n",
    "                error = ratings[i, j] - np.dot(P[i, :], Q[j, :].T)\n",
    "                for k in range(num_features):\n",
    "                    P[i, k] += learning_rate * (error * Q[j, k] - reg_param * P[i, k])\n",
    "                    Q[j, k] += learning_rate * (error * P[i, k] - reg_param * Q[j, k])\n",
    "\n",
    "    # Obliczanie straty \n",
    "    loss = 0\n",
    "    for i in range(num_users):\n",
    "        for j in range(num_items):\n",
    "            if not np.isnan(ratings[i, j]):\n",
    "                loss += (ratings[i, j] - np.dot(P[i, :], Q[j, :].T))**2\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Wynikowa macierz\n",
    "predicted_ratings = np.dot(P, Q.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodano kod testujący"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Wczytanie danych\n",
    "ratings_train = pd.read_csv('ml-100k/u1.base', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "ratings_test = pd.read_csv('ml-100k/u1.test', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "# Parametry macierzy\n",
    "num_users = ratings_train['user_id'].nunique()\n",
    "num_items = ratings_train['item_id'].nunique()\n",
    "num_features = 10  # Liczba ukrytych wymiarów\n",
    "learning_rate = 0.01\n",
    "reg_param = 0.1  # Parametr regularyzacji\n",
    "epochs = 100\n",
    "\n",
    "# Tworzenie macierzy użytkownik-film\n",
    "train_matrix = ratings_train.pivot(index='user_id', columns='item_id', values='rating')\n",
    "train_matrix = train_matrix.to_numpy()\n",
    "\n",
    "# Inicjalizacja macierzy P i Q\n",
    "P = np.random.normal(scale=1./num_features, size=(num_users, num_features))\n",
    "Q = np.random.normal(scale=1./num_features, size=(num_items, num_features))\n",
    "\n",
    "# Faktoryzacja macierzy (SGD)\n",
    "for epoch in range(epochs):\n",
    "    for i in range(num_users):\n",
    "        for j in range(num_items):\n",
    "            if not np.isnan(train_matrix[i, j]):  # Pomijamy brakujące oceny\n",
    "                error = train_matrix[i, j] - np.dot(P[i, :], Q[j, :].T)\n",
    "                for k in range(num_features):\n",
    "                    P[i, k] += learning_rate * (error * Q[j, k] - reg_param * P[i, k])\n",
    "                    Q[j, k] += learning_rate * (error * P[i, k] - reg_param * Q[j, k])\n",
    "\n",
    "# Wynikowa macierz\n",
    "predicted_ratings = np.dot(P, Q.T)\n",
    "\n",
    "# Sprawdzanie jakości na zbiorze testowym\n",
    "test_ratings = []\n",
    "predicted_test_ratings = []\n",
    "\n",
    "for _, row in ratings_test.iterrows():\n",
    "    user = row['user_id'] - 1  # Indeksy w zbiorze zaczynają się od 1\n",
    "    item = row['item_id'] - 1\n",
    "    true_rating = row['rating']\n",
    "    predicted_rating = predicted_ratings[user, item]\n",
    "    \n",
    "    test_ratings.append(true_rating)\n",
    "    predicted_test_ratings.append(predicted_rating)\n",
    "\n",
    "# Obliczanie błędu MSE\n",
    "mse = mean_squared_error(test_ratings, predicted_test_ratings)\n",
    "print(f\"Mean Squared Error (MSE) na zbiorze testowym: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
